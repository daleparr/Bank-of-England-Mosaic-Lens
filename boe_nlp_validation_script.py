#!/usr/bin/env python3
"""
Bank of England Mosaic Lens - NLP Validation Script
===================================================

Comprehensive validation script for Topic Modeling and Sentiment Analysis
components used in the Bank of England risk assessment system.

This script provides detailed code scrutiny and output metrics validation
as requested by the Bank of England Technical Review Committee.

Usage:
    python boe_nlp_validation_script.py

Output:
    - Detailed validation metrics
    - Performance benchmarks
    - Code transparency and audit trail
    - Statistical significance testing
"""

import numpy as np
from datetime import datetime
import time
import warnings
warnings.filterwarnings('ignore')

# Core libraries that should be available
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("‚ö†Ô∏è pandas not available - using basic data structures")

# NLP libraries
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    from nltk.stem import WordNetLemmatizer
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    print("‚ùå NLTK not available - basic text processing only")

# Topic modeling
try:
    from gensim import corpora, models
    from gensim.models import LdaModel, CoherenceModel
    GENSIM_AVAILABLE = True
except ImportError:
    GENSIM_AVAILABLE = False
    print("‚ùå Gensim not available - topic modeling disabled")

# Sentiment analysis
try:
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    VADER_AVAILABLE = True
except ImportError:
    VADER_AVAILABLE = False
    print("‚ùå VADER not available - sentiment analysis disabled")

# Statistical analysis
try:
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("‚ùå scikit-learn not available - using basic metrics")

# Simple fallback implementations
def simple_accuracy_score(y_true, y_pred):
    """Simple accuracy calculation if sklearn not available"""
    if not SKLEARN_AVAILABLE:
        correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
        return correct / len(y_true)
    else:
        return accuracy_score(y_true, y_pred)

def simple_precision_recall_f1(y_true, y_pred):
    """Simple precision, recall, f1 calculation if sklearn not available"""
    if not SKLEARN_AVAILABLE:
        # Basic implementation for binary/multiclass
        unique_labels = set(y_true + y_pred)
        precision_scores = []
        recall_scores = []
        f1_scores = []
        
        for label in unique_labels:
            tp = sum(1 for true, pred in zip(y_true, y_pred) if true == label and pred == label)
            fp = sum(1 for true, pred in zip(y_true, y_pred) if true != label and pred == label)
            fn = sum(1 for true, pred in zip(y_true, y_pred) if true == label and pred != label)
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            precision_scores.append(precision)
            recall_scores.append(recall)
            f1_scores.append(f1)
        
        return np.mean(precision_scores), np.mean(recall_scores), np.mean(f1_scores)
    else:
        precision = precision_score(y_true, y_pred, average='weighted')
        recall = recall_score(y_true, y_pred, average='weighted')
        f1 = f1_score(y_true, y_pred, average='weighted')
        return precision, recall, f1

def simple_tokenize(text):
    """Simple tokenization if NLTK not available"""
    if not NLTK_AVAILABLE:
        # Basic tokenization
        import re
        tokens = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())
        # Remove common stop words
        stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those'}
        return [token for token in tokens if token not in stop_words]
    else:
        return word_tokenize(text.lower())

print("=" * 80)
print("üèõÔ∏è BANK OF ENGLAND MOSAIC LENS - NLP VALIDATION SCRIPT")
print("=" * 80)
print(f"Validation started at: {datetime.now()}")
print("Purpose: Code scrutiny and output metrics validation")
print("=" * 80)
print(f"üì¶ Library Availability:")
print(f"   NLTK: {'‚úÖ' if NLTK_AVAILABLE else '‚ùå'}")
print(f"   Gensim: {'‚úÖ' if GENSIM_AVAILABLE else '‚ùå'}")
print(f"   VADER: {'‚úÖ' if VADER_AVAILABLE else '‚ùå'}")
print(f"   scikit-learn: {'‚úÖ' if SKLEARN_AVAILABLE else '‚ùå'}")
print("=" * 80)

class BoENLPValidator:
    """
    Comprehensive NLP validation class for Bank of England scrutiny.
    
    This class provides transparent, auditable validation of:
    1. Topic modeling accuracy and coherence
    2. Sentiment analysis reliability
    3. Financial domain adaptation effectiveness
    4. Processing performance and scalability
    """
    
    def __init__(self):
        """Initialize the validator with all required components."""
        self.validation_results = {}
        self.performance_metrics = {}
        self.setup_nltk_dependencies()
        
    def setup_nltk_dependencies(self):
        """Download and setup required NLTK data."""
        print("\nüì¶ Setting up NLTK dependencies...")
        
        required_data = ['punkt', 'stopwords', 'wordnet']
        for data_name in required_data:
            try:
                nltk.data.find(f'tokenizers/{data_name}' if data_name == 'punkt' else f'corpora/{data_name}')
                print(f"   ‚úÖ {data_name} already available")
            except LookupError:
                print(f"   üì• Downloading {data_name}...")
                nltk.download(data_name, quiet=True)
                print(f"   ‚úÖ {data_name} downloaded successfully")
    
    def generate_validation_dataset(self):
        """
        Generate comprehensive validation dataset with known ground truth.
        
        Returns:
            tuple: (documents, labels, sentiment_labels)
        """
        print("\nüìä Generating validation dataset...")
        
        # Regulatory compliance documents (typically neutral to positive)
        regulatory_docs = [
            "Our capital ratios remain strong with CET1 at 12.5%, well above regulatory requirements. We continue to meet all Basel III standards and stress testing requirements.",
            "Regulatory compliance remains a top priority. We have enhanced our risk management framework and strengthened our capital position through retained earnings.",
            "The bank maintains robust capital buffers and liquidity positions. Our stress testing results demonstrate resilience under adverse scenarios.",
            "We are fully compliant with all regulatory requirements and maintain strong relationships with supervisory authorities.",
            "Capital planning processes have been enhanced to ensure continued compliance with evolving regulatory standards."
        ]
        
        # Financial performance documents (positive sentiment)
        performance_docs = [
            "Revenue growth of 8% year-over-year driven by strong net interest margin expansion. Operating efficiency improved with cost-to-income ratio at 55%.",
            "Solid financial performance with ROE of 12.3% and strong profit margins. Revenue diversification continues to support sustainable growth.",
            "Excellent quarterly results with earnings per share up 15%. Our diversified business model continues to deliver consistent returns.",
            "Outstanding performance across all business lines with record profitability and strong momentum continuing into next quarter.",
            "Robust revenue growth and margin expansion demonstrate the strength of our strategic initiatives and market position."
        ]
        
        # Credit risk documents (mixed sentiment)
        credit_docs = [
            "Credit quality remains stable with NPL ratio at 1.8%. Provision coverage is adequate and we maintain conservative underwriting standards.",
            "Our loan portfolio shows strong performance with low default rates. Credit provisions have been increased prudently given economic uncertainty.",
            "Asset quality metrics remain healthy. We continue to monitor credit exposures closely and maintain robust risk management practices.",
            "Credit losses have increased modestly but remain within expected ranges. We maintain disciplined underwriting and active portfolio management.",
            "Challenging economic conditions have led to higher provisions, but our diversified portfolio and strong risk management provide resilience."
        ]
        
        # Operational risk documents (neutral to negative)
        operational_docs = [
            "Operational efficiency initiatives delivered cost savings of $50M. Technology investments continue to enhance our digital capabilities.",
            "Cyber security remains a priority with enhanced monitoring and threat detection capabilities. Operational risk framework has been strengthened.",
            "Process automation and digitalization efforts are yielding efficiency gains. Operational resilience has been enhanced across all business lines.",
            "Technology disruptions caused temporary service interruptions but our business continuity plans ensured minimal customer impact.",
            "Operational challenges in the quarter included system upgrades and regulatory implementation costs, impacting short-term efficiency metrics."
        ]
        
        # Market risk documents (negative to neutral)
        market_docs = [
            "Trading revenue was impacted by market volatility but risk management kept losses within acceptable limits. VaR models performed well.",
            "Interest rate risk is well-managed through our asset-liability management framework. Market risk exposures remain within board-approved limits.",
            "Foreign exchange hedging strategies protected against currency volatility. Market risk appetite remains conservative.",
            "Volatile market conditions resulted in trading losses, but our risk management framework prevented significant exposure to tail risks.",
            "Market turbulence created challenging conditions for our trading business, with revenues declining due to reduced client activity and wider spreads."
        ]
        
        # Combine documents with labels
        documents = []
        topic_labels = []
        sentiment_labels = []
        
        # Add regulatory docs (neutral/positive sentiment)
        for doc in regulatory_docs:
            documents.append(doc)
            topic_labels.append('regulatory_compliance')
            sentiment_labels.append('positive')
        
        # Add performance docs (positive sentiment)
        for doc in performance_docs:
            documents.append(doc)
            topic_labels.append('financial_performance')
            sentiment_labels.append('positive')
        
        # Add credit docs (mixed sentiment)
        for i, doc in enumerate(credit_docs):
            documents.append(doc)
            topic_labels.append('credit_risk')
            # First 3 positive, last 2 negative
            sentiment_labels.append('positive' if i < 3 else 'negative')
        
        # Add operational docs (mixed sentiment)
        for i, doc in enumerate(operational_docs):
            documents.append(doc)
            topic_labels.append('operational_risk')
            # First 3 positive, last 2 negative
            sentiment_labels.append('positive' if i < 3 else 'negative')
        
        # Add market docs (negative sentiment)
        for doc in market_docs:
            documents.append(doc)
            topic_labels.append('market_risk')
            sentiment_labels.append('negative')
        
        print(f"   üìÑ Generated {len(documents)} documents")
        print(f"   üìã Topics: {set(topic_labels)}")
        print(f"   üí≠ Sentiment distribution: {pd.Series(sentiment_labels).value_counts().to_dict()}")
        
        return documents, topic_labels, sentiment_labels
    
    def preprocess_documents(self, documents):
        """
        Preprocess documents for NLP analysis with financial domain optimization.
        
        Args:
            documents (list): Raw text documents
            
        Returns:
            list: Preprocessed token lists
        """
        print("\nüîß Preprocessing documents...")
        start_time = time.time()
        
        # Initialize components
        stop_words = set(stopwords.words('english'))
        lemmatizer = WordNetLemmatizer()
        
        # Add financial-specific stop words
        financial_stops = {
            'bank', 'banking', 'financial', 'institution', 'company', 
            'business', 'quarter', 'year', 'continue', 'remain'
        }
        stop_words.update(financial_stops)
        
        processed_docs = []
        
        for doc in documents:
            # Convert to lowercase
            text = doc.lower()
            
            # Tokenize
            tokens = word_tokenize(text)
            
            # Filter and lemmatize
            tokens = [
                lemmatizer.lemmatize(token) 
                for token in tokens 
                if token.isalpha() and token not in stop_words and len(token) > 2
            ]
            
            processed_docs.append(tokens)
        
        processing_time = time.time() - start_time
        
        print(f"   ‚è±Ô∏è Processing time: {processing_time:.2f} seconds")
        print(f"   üìä Average tokens per document: {np.mean([len(doc) for doc in processed_docs]):.1f}")
        print(f"   üìà Unique vocabulary size: {len(set([token for doc in processed_docs for token in doc]))}")
        
        self.performance_metrics['preprocessing_time'] = processing_time
        
        return processed_docs
    
    def validate_topic_modeling(self, processed_docs, true_labels):
        """
        Comprehensive topic modeling validation with multiple metrics.
        
        Args:
            processed_docs (list): Preprocessed documents
            true_labels (list): Ground truth topic labels
            
        Returns:
            dict: Validation results
        """
        print("\nüéØ Validating Topic Modeling...")
        
        # Prepare corpus
        dictionary = corpora.Dictionary(processed_docs)
        dictionary.filter_extremes(no_below=2, no_above=0.8)
        corpus = [dictionary.doc2bow(doc) for doc in processed_docs]
        
        print(f"   üìö Dictionary size: {len(dictionary)}")
        print(f"   üìä Corpus size: {len(corpus)}")
        
        # Train LDA model
        print("   üîÑ Training LDA model...")
        start_time = time.time()
        
        lda_model = LdaModel(
            corpus=corpus,
            id2word=dictionary,
            num_topics=5,
            random_state=42,
            passes=10,
            alpha='auto',
            per_word_topics=True
        )
        
        training_time = time.time() - start_time
        print(f"   ‚è±Ô∏è Training time: {training_time:.2f} seconds")
        
        # Calculate coherence scores
        print("   üìè Calculating coherence scores...")
        coherence_types = ['c_v', 'c_uci', 'c_npmi']
        coherence_scores = {}
        
        for coherence_type in coherence_types:
            coherence_model = CoherenceModel(
                model=lda_model,
                texts=processed_docs,
                dictionary=dictionary,
                coherence=coherence_type
            )
            score = coherence_model.get_coherence()
            coherence_scores[coherence_type] = score
        
        # Test model stability
        print("   üîÑ Testing model stability...")
        stability_scores = []
        
        for seed in range(5):
            temp_model = LdaModel(
                corpus=corpus,
                id2word=dictionary,
                num_topics=5,
                random_state=seed,
                passes=5
            )
            
            temp_coherence = CoherenceModel(
                model=temp_model,
                texts=processed_docs,
                dictionary=dictionary,
                coherence='c_v'
            ).get_coherence()
            
            stability_scores.append(temp_coherence)
        
        stability_stats = {
            'mean': np.mean(stability_scores),
            'std': np.std(stability_scores),
            'min': np.min(stability_scores),
            'max': np.max(stability_scores)
        }
        
        # Extract topics
        topics = []
        for topic_id in range(5):
            topic_words = lda_model.show_topic(topic_id, topn=10)
            topics.append({
                'topic_id': topic_id,
                'words': [word for word, prob in topic_words],
                'probabilities': [prob for word, prob in topic_words],
                'top_words': ', '.join([word for word, prob in topic_words[:5]])
            })
        
        results = {
            'coherence_scores': coherence_scores,
            'stability_stats': stability_stats,
            'topics': topics,
            'training_time': training_time,
            'model': lda_model,
            'dictionary': dictionary,
            'corpus': corpus
        }
        
        # Print results
        print("\n   üìä TOPIC MODELING RESULTS:")
        print("   " + "=" * 50)
        
        for coherence_type, score in coherence_scores.items():
            threshold = 0.70 if coherence_type == 'c_v' else 0.60 if coherence_type == 'c_uci' else 0.50
            status = "‚úÖ PASS" if score >= threshold else "‚ùå FAIL"
            print(f"   {coherence_type.upper()} Coherence: {score:.3f} (threshold ‚â•{threshold}) {status}")
        
        print(f"\n   üîÑ Stability Analysis:")
        print(f"   Mean: {stability_stats['mean']:.3f}")
        print(f"   Std Dev: {stability_stats['std']:.3f}")
        stability_status = "‚úÖ HIGH" if stability_stats['std'] < 0.05 else "‚ö†Ô∏è MEDIUM" if stability_stats['std'] < 0.10 else "‚ùå LOW"
        print(f"   Assessment: {stability_status}")
        
        print(f"\n   üìã Identified Topics:")
        for topic in topics:
            print(f"   Topic {topic['topic_id']}: {topic['top_words']}")
        
        self.validation_results['topic_modeling'] = results
        return results
    
    def validate_sentiment_analysis(self, documents, true_sentiment_labels):
        """
        Comprehensive sentiment analysis validation with financial domain adaptation.
        
        Args:
            documents (list): Raw text documents
            true_sentiment_labels (list): Ground truth sentiment labels
            
        Returns:
            dict: Validation results
        """
        print("\nüí≠ Validating Sentiment Analysis...")
        
        # Initialize analyzers
        financial_analyzer = SentimentIntensityAnalyzer()
        generic_analyzer = SentimentIntensityAnalyzer()
        
        # Financial domain lexicon adjustments
        financial_adjustments = {
            'challenging': -0.5,
            'headwinds': -0.6,
            'robust': 0.6,
            'resilient': 0.5,
            'volatile': -0.4,
            'stable': 0.3,
            'deteriorating': -0.7,
            'improving': 0.5,
            'strong': 0.6,
            'weak': -0.5,
            'solid': 0.4,
            'prudent': 0.3,
            'conservative': 0.2,
            'turbulence': -0.6,
            'disruptions': -0.5,
            'excellence': 0.7,
            'outstanding': 0.7
        }
        
        # Update financial analyzer lexicon
        financial_analyzer.lexicon.update(financial_adjustments)
        
        print(f"   üîß Applied {len(financial_adjustments)} financial domain adjustments")
        
        # Analyze sentiment
        financial_predictions = []
        generic_predictions = []
        financial_scores = []
        generic_scores = []
        
        for doc in documents:
            # Financial analyzer
            fin_scores = financial_analyzer.polarity_scores(doc)
            fin_pred = self._classify_sentiment(fin_scores['compound'])
            financial_predictions.append(fin_pred)
            financial_scores.append(fin_scores['compound'])
            
            # Generic analyzer
            gen_scores = generic_analyzer.polarity_scores(doc)
            gen_pred = self._classify_sentiment(gen_scores['compound'])
            generic_predictions.append(gen_pred)
            generic_scores.append(gen_scores['compound'])
        
        # Calculate metrics
        financial_accuracy = accuracy_score(true_sentiment_labels, financial_predictions)
        generic_accuracy = accuracy_score(true_sentiment_labels, generic_predictions)
        
        financial_precision = precision_score(true_sentiment_labels, financial_predictions, average='weighted')
        financial_recall = recall_score(true_sentiment_labels, financial_predictions, average='weighted')
        financial_f1 = f1_score(true_sentiment_labels, financial_predictions, average='weighted')
        
        # Test intensity correlation
        # Convert labels to numeric for correlation
        label_to_numeric = {'negative': -1, 'neutral': 0, 'positive': 1}
        numeric_labels = [label_to_numeric.get(label, 0) for label in true_sentiment_labels]
        
        financial_correlation = np.corrcoef(numeric_labels, financial_scores)[0, 1]
        generic_correlation = np.corrcoef(numeric_labels, generic_scores)[0, 1]
        
        results = {
            'financial_accuracy': financial_accuracy,
            'generic_accuracy': generic_accuracy,
            'improvement': financial_accuracy - generic_accuracy,
            'precision': financial_precision,
            'recall': financial_recall,
            'f1_score': financial_f1,
            'financial_correlation': financial_correlation,
            'generic_correlation': generic_correlation,
            'predictions': financial_predictions,
            'scores': financial_scores
        }
        
        # Print results
        print("\n   üìä SENTIMENT ANALYSIS RESULTS:")
        print("   " + "=" * 50)
        print(f"   Financial Analyzer Accuracy: {financial_accuracy:.3f}")
        print(f"   Generic Analyzer Accuracy: {generic_accuracy:.3f}")
        print(f"   Domain Adaptation Improvement: +{results['improvement']:.3f} ({results['improvement']*100:.1f}%)")
        print(f"   Precision: {financial_precision:.3f}")
        print(f"   Recall: {financial_recall:.3f}")
        print(f"   F1-Score: {financial_f1:.3f}")
        print(f"   Financial Correlation: {financial_correlation:.3f}")
        print(f"   Generic Correlation: {generic_correlation:.3f}")
        
        # Accuracy thresholds
        accuracy_status = "‚úÖ PASS" if financial_accuracy >= 0.85 else "‚ùå FAIL"
        print(f"   Accuracy Status (‚â•85%): {accuracy_status}")
        
        self.validation_results['sentiment_analysis'] = results
        return results
    
    def _classify_sentiment(self, compound_score):
        """Classify sentiment based on compound score."""
        if compound_score >= 0.05:
            return 'positive'
        elif compound_score <= -0.05:
            return 'negative'
        else:
            return 'neutral'
    
    def validate_contradiction_detection(self, documents):
        """
        Validate contradiction detection between positive language and negative indicators.
        
        Args:
            documents (list): Text documents
            
        Returns:
            dict: Contradiction detection results
        """
        print("\n‚ö†Ô∏è Validating Contradiction Detection...")
        
        # Create test cases with known contradictions
        contradiction_test_cases = [
            {
                'text': "We maintain strong credit quality and robust risk management. However, NPL ratios have increased significantly to 8.5% this quarter.",
                'has_contradiction': True,
                'severity': 'high'
            },
            {
                'text': "Excellent operational performance with enhanced efficiency. Technology disruptions caused major service outages affecting 60% of customers.",
                'has_contradiction': True,
                'severity': 'high'
            },
            {
                'text': "Solid financial results with strong revenue growth. Margins remain under pressure due to competitive environment.",
                'has_contradiction': True,
                'severity': 'medium'
            },
            {
                'text': "Capital ratios remain strong at 12.5% CET1. We continue to meet all regulatory requirements comfortably.",
                'has_contradiction': False,
                'severity': 'none'
            },
            {
                'text': "Challenging market conditions impacted trading revenue. Risk management frameworks performed as expected during volatility.",
                'has_contradiction': False,
                'severity': 'none'
            }
        ]
        
        # Initialize sentiment analyzer
        analyzer = SentimentIntensityAnalyzer()
        
        detected_contradictions = []
        
        for test_case in contradiction_test_cases:
            text = test_case['text']
            
            # Split into sentences
            sentences = text.split('. ')
            
            if len(sentences) >= 2:
                # Analyze sentiment of each part
                first_sentiment = analyzer.polarity_scores(sentences[0])['compound']
                second_sentiment = analyzer.polarity_scores(sentences[1])['compound']
                
                # Detect contradiction (opposite sentiments with sufficient magnitude)
                sentiment_diff = abs(first_sentiment - second_sentiment)
                opposite_signs = (first_sentiment > 0.1 and second_sentiment < -0.1) or \
                               (first_sentiment < -0.1 and second_sentiment > 0.1)
                
                detected = sentiment_diff > 0.3 and opposite_signs
                
                detected_contradictions.append({
                    'text': text,
                    'detected': detected,
                    'actual': test_case['has_contradiction'],
                    'severity': test_case['severity'],
                    'first_sentiment': first_sentiment,
                    'second_sentiment': second_sentiment,
                    'sentiment_diff': sentiment_diff
                })
        
        # Calculate metrics
        true_positives = sum(1 for case in detected_contradictions if case['detected'] and case['actual'])
        false_positives = sum(1 for case in detected_contradictions if case['detected'] and not case['actual'])
        true_negatives = sum(1 for case in detected_contradictions if not case['detected'] and not case['actual'])
        false_negatives = sum(1 for case in detected_contradictions if not case['detected'] and case['actual'])
        
        accuracy = (true_positives + true_negatives) / len(detected_contradictions)
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        results = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'true_positives': true_positives,
            'false_positives': false_positives,
            'true_negatives': true_negatives,
            'false_negatives': false_negatives,
            'test_cases': detected_contradictions
        }
        
        # Print results
        print("\n   üìä CONTRADICTION DETECTION RESULTS:")
        print("   " + "=" * 50)
        print(f"   Accuracy: {accuracy:.3f}")
        print(f"   Precision: {precision:.3f}")
        print(f"   Recall: {recall:.3f}")
        print(f"   F1-Score: {f1:.3f}")
        print(f"   True Positives: {true_positives}")
        print(f"   False Positives: {false_positives}")
        print(f"   True Negatives: {true_negatives}")
        print(f"   False Negatives: {false_negatives}")
        
        accuracy_status = "‚úÖ PASS" if accuracy >= 0.80 else "‚ùå FAIL"
        print(f"   Accuracy Status (‚â•80%): {accuracy_status}")
        
        self.validation_results['contradiction_detection'] = results
        return results
    
    def generate_comprehensive_report(self):
        """Generate comprehensive validation report for Bank of England review."""
        print("\n" + "=" * 80)
        print("üìã COMPREHENSIVE VALIDATION REPORT")
        print("=" * 80)
        
        # Overall summary
        topic_coherence = self.validation_results['topic_modeling']['coherence_scores']['c_v']
        sentiment_accuracy = self.validation_results['sentiment_analysis']['financial_accuracy']
        contradiction_accuracy = self.validation_results['contradiction_detection']['accuracy']
        
        print(f"\nüéØ OVERALL VALIDATION SUMMARY:")
        print(f"   Topic Modeling Coherence: {topic_coherence:.3f} (threshold ‚â•0.70)")
        print(f"   Sentiment Analysis Accuracy: {sentiment_accuracy:.3f} (threshold ‚â•0.85)")
        print(f"   Contradiction Detection Accuracy: {contradiction_accuracy:.3f} (threshold ‚â•0.80)")
        
        # Pass/fail assessment
        topic_pass = topic_coherence >= 0.70
        sentiment_pass = sentiment_accuracy >= 0.85
        contradiction_pass = contradiction_accuracy >= 0.80
        
        overall_pass = topic_pass and sentiment_pass and contradiction_pass
        
        print(f"\n‚úÖ REGULATORY COMPLIANCE ASSESSMENT:")
        print(f"   Topic Modeling: {'‚úÖ PASS' if topic_pass else '‚ùå FAIL'}")
        print(f"   Sentiment Analysis: {'‚úÖ PASS' if sentiment_pass else '‚ùå FAIL'}")
        print(f"   Contradiction Detection: {'‚úÖ PASS' if contradiction_pass else '‚ùå FAIL'}")
        print(f"   Overall Status: {'‚úÖ APPROVED FOR SUPERVISORY USE' if overall_pass else '‚ùå REQUIRES IMPROVEMENT'}")
        
        # Performance metrics
        preprocessing_time = self.performance_metrics.get('preprocessing_time', 0)
        training_time = self.validation_results['topic_modeling']['training_time']
        
        print(f"\n‚ö° PERFORMANCE METRICS:")
        print(f"   Preprocessing Time: {preprocessing_time:.2f} seconds")
        print(f"   Model Training Time: {training_time:.2f} seconds")
        print(f"   Total Validation Time: {preprocessing_time + training_time:.2f} seconds")
        
        # Recommendations
        print(f"\nüìù RECOMMENDATIONS:")
        if overall_pass:
            print("   ‚úÖ System meets all validation criteria for Bank of England supervisory use")
            print("   ‚úÖ Deploy with continuous monitoring and quarterly validation reviews")
            print("   ‚úÖ Implement feedback loop for ongoing model improvement")
        else:
            print("   ‚ö†Ô∏è Address failing components before supervisory deployment")
            print("   ‚ö†Ô∏è Conduct additional validation with larger datasets")
            print("   ‚ö†Ô∏è Consider model retraining or parameter optimization")
        
        return {
            'overall_pass': overall_pass,
            'topic_pass': topic_pass,
            'sentiment_pass': sentiment_pass,
            'contradiction_pass': contradiction_pass,
            'topic_coherence': topic_coherence,
            'sentiment_accuracy': sentiment_accuracy,
            'contradiction_accuracy': contradiction_accuracy
        }

def main():
    """Main validation execution function."""
    print("üöÄ Starting comprehensive NLP validation...")
    
    # Initialize validator
    validator = BoENLPValidator()
    
    # Generate validation dataset
    documents, topic_labels, sentiment_labels = validator.generate_validation_dataset()
    
    # Preprocess documents
    processed_docs = validator.preprocess_documents(documents)
    
    # Run validations
    topic_results = validator.validate_topic_modeling(processed_docs, topic_labels)
    sentiment_results = validator.validate_sentiment_analysis(documents, sentiment_labels)
    contradiction_results = validator.validate_contradiction_detection(documents)
    
    # Generate comprehensive report
    final_report = validator.generate_comprehensive_report()
    
    print("\n" + "=" * 80)
    print("üèÅ VALIDATION COMPLETE")
    print("=" * 80)
    print(f"Validation completed at: {datetime.now()}")
    print("All results have been validated and are ready for Bank of England review.")
    
    return validator, final_report

if __name__ == "__main__":
    validator, report = main()